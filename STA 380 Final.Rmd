---
title: "STA380 II"
author: "Akshat Johari, Kolton Fowler, Rushiil Deshmukh, Troy Richard"
date: "08/16/2021"
output: 
  md_document:
    variant: markdown_github
    
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
options(warn=-1)
```

## Github link for this project: https://github.com/BashfulBaobab/ML-Project-Repo

## R Markdown

STA380 Part II 

# Question 1 - Green Buildings

```{r 1}
rm(list=ls())
library(glmnet)
library(ggplot2)
data1=read.csv('greenbuildings.csv')
summary(data1)
attach(data1)
```

## Part 1
Dropping 10% low occupancy

```{r}

```

```{r 1_1_1}
ggplot(data1,aes(x = leasing_rate)) + geom_histogram(binwidth=11,color="black", fill="white") +
  labs(x='Leasing rate', y='Number', title = 'Leasing rate count')+ 
  geom_vline(aes(xintercept=median(leasing_rate)),
              color="red", linetype="dashed", size=1)+
  geom_vline(aes(xintercept=mean(leasing_rate)),
             color="blue", linetype="dashed", size=1)
```

```{r}

```

We found the median leasing rate to be ~90%, and the buildings with the lowest occupancy were highly skewed toward 0%, showing there is likely a variable not included in this data set that is impacting the buildings with the lowest occupancy rates. Further, considering 0% occupancy is quite far from the 90% median, it can be concluded that dropping the bottom 10% of buildings based on occupancy will likely not impact the model, and could even help increase its accuracy.

```{r 1_1_2}
lowoccup=data1[data1$leasing_rate == 0,]
lowoccup=na.omit(lowoccup)
ggplot(lowoccup, aes(renovated, ..count..)) + geom_bar()+
  labs(x='Renovated', y='Count', title = '0 occup,renovated count')

lowoccup2=data1[data1$leasing_rate < 0.1,]
lowoccup2=na.omit(lowoccup)
summary(lowoccup2)
```
There appear to be a notable number of buildings with 0% occupancy.
On further analysis, it is apparent that the median age for low occupancy buildings is 57 as opposed to 34 for the main data set, as well as a lower number of renovated buildings among the buildings with lower occupancy. This could provide insight into the variables impacting the lowest 10% of buildings based on occupancy rate, such as ongoing renovation projects or lack of permit for occupancy due to hazardous conditions in these older buildings. This is a better explanation to account for the dropping of the bottom 10% of buildings as opposed to simply stating "something weird" is happening with them. 

## Part 2
To test for a premium in rent for green buildings, we cannot simply subtract the difference between the two median rents among green and non-green buildings as we are not accounting for different factors that also affect revenue. To test if there is a true difference in price per square-foot, ideally, we would run a regression and hold other variables constant to see the individual effect on rent for a green building versus a non-green building.

```{r 1_2}
plot(x = Electricity_Costs, y = Rent, main = "Rent vs Electricity Cost")
```
```{r 1_2_2}
Energystar = as.factor(Energystar)
a = aggregate(amenities ~ Energystar, data1, mean)
barplot(a$amenities, names.arg = a$Energystar, xlab = "EnergyStar", ylab = "Amenities", col = rainbow(2),main = 'Amenities for Energy star')
```

## Part 3
As seen above, as rent increases, electricity costs also increase, therefore one cannot simply assume a direct increase in profit with the increase in rent for green buildings. Furthermore, energy efficient buildings are more likely to have amenities. Both of these variables will increase overhead costs, further justifying a lack of certainty in a profit stemming from increased rent revenue generation in green buildings.

## Part 4
The calculation for $5 million in extra construction costs for a green building is valid, and as we previously stated, the Median occupancy rate is 90%, therefore it is safe to take a 90% test occupancy rate for recuperation calculations. This however does not mean the recuperation calculations are accurate due to variables not accounted for such as increased amenity and electricity costs, as explained above.

## Part 5
```{r 1_5}
ggplot(data1,aes(x = factor(renovated), y = age)) + 
  geom_bar(stat='summary') + 
  labs(x = 'Renovated Buildings')
  
```

```{r}

```

The stats guru assumes that the building will be earning rent for 30 years. This assumption is accurate since the average age for non-renovated buildings is 35. Therefore, the building will likely generate rent revenue for at least 30 years before undergoing a renovation. However, we cannot confirm that this is a 
financially good decision based purely on the stats guru's conclusions, due to the presence of multiple confounding variables that are unaccounted for, as depicted in part 4.


## Part 6
```{r 1_6_1}
ggplot(data = data1, aes(x = Energystar, y = class_a, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```
```{r 1_6_2}
ggplot(data = data1, aes(x = Energystar, y = class_b, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```

```{r 1_6_3}
ggplot(data = data1, aes(x = amenities, y = class_a, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```

```{r 1_6_4}
ggplot(data = data1, aes(x = amenities, y = class_b, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```
```{r 1_6_5}
ggplot(data1, aes(x = factor(green_rating), y = age)) + 
  geom_bar(stat = 'summary') +
  labs(x = 'Green Rating')
```

```{r 1_6_6}

```
As seen above, the rent in a green status building can also be impacted by other variables beyond its green status. 
For example, building class type is correlated with green status. Green status buildings are more likely to be Class A buildings, and Class A buildings tend to experience an increase in amenities. So, it could be said that beyond green status, the rent of a green building is impacted by other variables such as amenities and class type. Some other confounding variables are Age and Employment Growth.

## Conclusion

For the majority of conclusions found by the stats guru, we are also unconvinced his findings are correct. The most common issue his work has is in not accounting for all variables that affect Rent. As described, we found several confounding variables in our data set that impact Rent of Green Buildings, so we know that the variables play off one another. Therefore, to create a model that accurately reflects the factors that impact the profit and financial attractiveness of this green building, all variables should be taken into account in a far deeper level than that which the stats guru attempted to do. 


# Question 2 - Flights at ABIA

```{r}
abia = read.csv('ABIA.csv')
```

```{r}
library(scales)
library(ggplot2)
```

The intended audience for our analysis is airport schedulers / passengers of air travel. The plots display delays by various time periods.

## Part 1
```{r Time of Day, warning=FALSE}
plot = ggplot(data=abia, aes(x=ArrTime, y=ArrDelay, group=1)) +
  ggtitle('Arrival Delay for Time of Day at AUS') +
  xlab('Arrival Time') + ylab('Arrival Delay in Minutes') + 
  labs(subtitle = '5-6 am has the lowest arrival delay, \n and range subsequently increases throughout the day') +
  scale_x_continuous(breaks= pretty_breaks()) +
  geom_line() +
  geom_point()
plot
plot1 = ggplot(data=abia, aes(x=DepTime, y=DepDelay, group=1)) +
  ggtitle('Departure Delay for Time of Day at AUS') +
  xlab('Departure Time') + ylab('Departure Delay in Minutes') + 
  labs(subtitle = 'Departure delay shows a similar trend - best times are early in the morning, \nand then they worsen throughout the day') +
  scale_x_continuous(breaks= pretty_breaks()) +
  geom_line() +
  geom_point()
plot1
```

## Part 2
```{r Day of Week}
dayWeek = aggregate(cbind(DepDelay, ArrDelay) ~ DayOfWeek, data = abia, FUN = mean)

ggplot(dayWeek, aes(x = DayOfWeek)) + 
  geom_line(aes(y = DepDelay, color = 'Departure Delay')) + 
  geom_line(aes(y = ArrDelay, color = 'Arrival Delay')) +
  labs(title = 'Delay by Day of Week', y = 'Delay', x = 'Day of Week') + 
  scale_color_manual('Metric', values=c('red', 'steelblue')) + 
  labs(subtitle = 'Average arrival and departure delay time in minutes by day of week') +
  scale_x_continuous(breaks= pretty_breaks()) +
  theme(legend.position = c(0.25, 0.85))
```

## Part 3
```{r Day of Month}
dayMonth = aggregate(cbind(DepDelay, ArrDelay) ~ DayofMonth, data = abia, FUN = mean)

ggplot(dayMonth, aes(x = DayofMonth)) + 
  geom_line(aes(y = DepDelay, color = 'Departure Delay')) + 
  geom_line(aes(y = ArrDelay, color = 'Arrival Delay')) +
  labs(title = 'Delay by Day of Month', y = 'Delay', x = 'Day of Month') + 
  labs(subtitle = 'Average arrival and departure delay time in minutes by day of month') +
  scale_color_manual('Metric', values=c('red', 'steelblue')) +
  scale_x_continuous(breaks= pretty_breaks()) + 
  theme(legend.position = c(0.2, 0.85))
```

## Part 4
```{r Month of Year}
monYear = aggregate(cbind(DepDelay, ArrDelay) ~ Month, data = abia, FUN = mean)

monYear$Month = as.integer(monYear$Month)


ggplot(monYear, aes(x = Month)) + 
  geom_line(aes(y = DepDelay, color = 'Departure Delay')) + 
  geom_line(aes(y = ArrDelay, color = 'Arrival Delay')) +
  labs(title = 'Delay by Month of Year', y = 'Delay', x = 'Month of Year') +
  scale_color_manual('Metric', values=c('red', 'steelblue')) + 
  labs(subtitle = 'Average arrival and departure delay time in minutes by month') +
  scale_x_continuous(breaks= pretty_breaks()) + 
  theme(legend.position = c(0.7, 0.85))

```
```{r}

```

## Part 5
This plot shows flights on time for the best 4 airlines, as determined by [The Points Guy](https://thepointsguy.com/news/tpg-2021-best-us-airlines/).

```{r Non delayed}

notLate = subset(abia, Cancelled == 0 & ArrDelay <= 0)
notLate = subset(notLate, UniqueCarrier %in% c("DL", "WN", "UA", "AA"))

plotearly = ggplot(notLate, aes(x=DayOfWeek)) +
geom_bar(width=.4, fill="steelblue") +
labs(title = "Non-delayed flights by day of week",
     subtitle = 'Early or timely flights by Alaska, Delta, United, Southwest',
x = "Day of Week",
y = "Number of Flights") +
theme(axis.text.x = element_text(angle=90, vjust=0.6))

plotearly + facet_grid(. ~ UniqueCarrier)

```


# Question 3 - Portfolio Modelling

```{r 3_1, message=FALSE}
rm(list=ls())
library(mosaic)
library(quantmod)
library(foreach)
```

Here is a short description of the ETFs we chose for this model:

- Equity- Large Cap Growth - SPY
- Equity- Small Cap Growth - IWM
- Large Cap- Blend - RSP
- Bonds- UST 1-3 YR - SHY
- Hedge Fund - DBEF

## Part 1
Adjusting all stocks
```{r 3_2, message=FALSE, warning=FALSE}
mystocks = c("SPY", "IWM", "RSP", "SHY", "DBEF")
myprices = getSymbols(mystocks, from = "2015-12-01")
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}
```


## Part 2 
Combine all the returns in a matrix
```{r 3_3}
all_returns = cbind( ClCl(SPYa),
                     ClCl(IWMa),
                     ClCl(RSPa),
                     ClCl(SHYa),
                     ClCl(DBEFa)
                     )
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```


## Part 3 
Calculating returns for each ETF
```{r 3_4}
returns = data.frame(round(dailyReturn(SPYa)*100,4),
                     round(dailyReturn(IWMa)*100,4),round(dailyReturn(RSPa)*100,4),
                     round(dailyReturn(SHYa)*100,4),round(dailyReturn(DBEFa)*100,4))

colnames(returns)=mystocks
head(returns,10)
cor(returns)
```

Now, let's figure out how to set their weights accordingly. For this, we 
calculate the daily returns for each ETF and find their the median and standard 
deviation to understand their mean returns and volatility.


## Part 4
Getting the median return and volatility for each ETF
```{r 3_5}
mystocks_Stats = do.call(data.frame, 
                    list(mean = round(apply(returns, 2, mean),4),
                         sd = round(apply(returns, 2, sd),4),
                         median = round(apply(returns, 2, median),4),
                         min = round(apply(returns, 2, min),4),
                         max = round(apply(returns, 2, max),4)))

t(mystocks_Stats)
```
As is visible from the above table, IWM has the highest volatility, followed by 
DBEF, RSP, SPY and finally SHY. This is congruent with our ETF behaviors so far.
The median value for DBEF is the highest, followed by IWM, RSP, SPY and SHP. 

## Part 5 - Modelling the Portfolios

### Portfolio 1 - Aggressive portfolio
```{r 3_6_1}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.175, 0.3, 0.175, 0, 0.35)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}
```

Profit/loss
```{r 3_6_1_1}

mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
```

5% value at risk:
```{r 3_6_1_2}

abs(quantile(sim1[,n_days]- initial_wealth, prob=0.05))
```


```{r 3_6_1_3}
hist(sim1[,n_days], 25, main = 'Histogram of Total Wealth Plot', xlab = 'Total Wealth')
```


```{r 3_6_1_4}
hist(sim1[,n_days]- initial_wealth, breaks=30, main = 'Histogram of Net Profit', xlab = 'Net Profit')
```

### Portfolio 2 - Balanced
``` {r 3_6_2}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

```

Profit/loss
```{r 3_6_2_1}
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
```

5% value at risk
```{r 3_6_2_2}
abs(quantile(sim1[,n_days]- initial_wealth, prob=0.05))
```


```{r 3_6_2_3}
hist(sim1[,n_days], 25, main = 'Total Wealth Plot', xlab = 'Total Wealth')
```


``` {r 3_6_2_4}
hist(sim1[,n_days]- initial_wealth, breaks=30, main = 'Histogram of Net Profit', xlab = 'Net Profit')
```

### Portfolio 3 - Safe
```{r 3_6_3}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.1, 0.1, 0.1, 0.6, 0.1)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}
```

Profit/loss
``` {r 3_6_3_1}
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
```

5% value at risk
```{r 3_6_3_2}
abs(quantile(sim1[,n_days]- initial_wealth, prob=0.05))
```


```{r 3_6_3_3}
hist(sim1[,n_days], 25, main = 'Total Wealth Plot', xlab = 'Total Wealth')
```

```{r 3_6_3_4}
hist(sim1[,n_days]- initial_wealth, breaks=30, main = 'Histogram of Net Profit', xlab = 'Net Profit')
```

## Conclusion

* Aggressive Portfolio:  
In creating our aggressive portfolio, we avoided the UST Bond completely as we selected this knowing it has a lower expected return and risk. We did so to create a balance of risk, but as this is an aggressive approach, we aren’t concerned with that here. But as to how we would weigh our Equity ETFs and Hedge fund ETF, we tested the standard deviation and median return for all our ETFs and then allocated the weight accordingly based on those metrics. We found the Hedge fund to have the lowest risk while also having the highest expected return, so we valued ‘DBEF’ at 35%. The next most efficient stock was IWM and we placed a 30% weight there. ‘SPY’ and ‘RSP’ were about equal in terms of both risk and reward, so we gave each of those the remaining 17.5%.

* Balanced Portfolio:  
We created a balanced portfolio and weighted each ETF equally at 20%. We found a profit of about $950 and a 5% VaR ≈ 6400. We did this in hope of getting baseline values and then being able to compare our safe and aggressive combinations.

* Safe Portfolio:  
We purposefully selected a UST Government Bond for our portfolio as those bonds are known to be “risk-free”, so when creating our Safe portfolio we assumed this would garner a large portion of the weight. To test this, we obtained the standard deviation and median return for all of our ETFs and then allocated the weight accordingly based on those metrics. Our portfolio is comprised of a combination of Small & Large Cap Growth Equities, a Value Equity fund, a Hedge Fund, and lastly the UST Government Bond. As we expected, the standard deviation and expected return was considerably less for the UST Bond than compared with the others so we placed 60% of our weight here. All the others had ranges similar to one another so in still wanting to be profitable we gave each Equity fund and the Hedge fund 10%. This portfolio combination had about $540 profit and our lowest 5% VaR of  ≈ 2900.

# Question 4 - Market Segmentation


```{r 4_1}
rm(list=ls())
library(tidyverse)
library(glmnet)
library(ggplot2)
library(reshape2)
library(cluster)
library(mvtnorm)
library(factoextra)
library(NbClust)
library(foreach)
library(LICORS)
library(HSAUR)
library(fpc)
library(corrplot)

s_main=read.csv('social_marketing.csv')
```


## Part 1

Removing unwanted columns and scaling the data
```{r 4_2}
s_1 = s_main[,2:36]
s_col = names(s_1) %in% c('','chatter','uncategorized','spam','adult')
s = s_main[,!s_col]
scaled_s=scale(s, center=TRUE, scale=TRUE)
```


## Part 2

Plotting the correlation between variables
```{r 4_3}
mtrix_corr=round(cor(s), 2)
corrplot(mtrix_corr, method="circle")
```


As is visible from this correlation plot, a lot of variables in this dataset
are highly correlated. Certain examples of this are:  
- Politics and travel  
- Parenting and sports  
- Health nutrition and Personal fitness  



## Part 3

Determining the optimum number of clusters

### Elbow method
```{r 4_4_1}
fviz_nbclust(scaled_s, kmeans, method = "wss")
```
We're unable to easily determine the right value of k from this plot alone. 
However, the line seems to stabilize after k=7.

### Silhouette method
```{r 4_4_2}
fviz_nbclust(scaled_s, kmeans, method = "silhouette")+labs(subtitle = "Silhouette method")
```


This plot shows optimum number of clusters at 2, but intuitively it seems too 
less. The silhouette width tends to stabilize at k=5, before further reducing at
k=8.

### CH method
```{r 4_4_3, warning=FALSE}
k_grid = seq(2,20,by=1)
N = nrow(scaled_s) 
CH_grid = foreach(k = k_grid, .combine='c') %do% { 
  cluster_k = kmeans(scaled_s, k, nstart=50) 
  W = cluster_k$tot.withinss 
  B = cluster_k$betweenss 
  CH = (B/W)*((N-k)/(k-1)) 
  CH 
} 
plot(k_grid, CH_grid)
```


The first dip appears to be at k=2, thereby again giving us 2 optimal clusters.


## Part 4

Principal Component Analysis  
We run PCA to reduce the number of correlated variables. This will allow us to
segment the data into the appropriate categories later on.  

Generating the PCA
```{r 4_5_1}
scaled_pca = prcomp(s, scale=TRUE, center = TRUE)
summary(scaled_pca)
plot(scaled_pca, type= 'l')
```

Calculating cumulative variance
```{r 4_5_2}
variance_pca=scaled_pca$sdev ^ 2
variance_pca_a=variance_pca/sum(variance_pca)
#Cumulative sum of variation explained
plot(cumsum(variance_pca_a), xlab = "Principal Component", 
     ylab = "Fraction of variance explained")
cumsum(variance_pca_a)[10]
```
By the Kaiser criterion, we should drop all principal components with
eigen values < 1.

Generating PCA data
```{r 4_5_3}
varimax(scaled_pca$rotation[, 1:11])$loadings
scores = scaled_pca$x
pc_data=as.data.frame(scores[,1:18])
S=pc_data
```



## Part 5

K means clustering  

After running PCA and identifying various plots, we chose to proceed with k-means
clustering method using 4 clusters and 25 starts. This is because 4 segments were
found to be the most practical in terms of data interpret ability and prevented
any significant overlap of cluster data.

Running K means

```{r 4_6_1}
fviz_nbclust(S, kmeans, method = "wss")

clust_k = kmeanspp(S, 4, nstart=25)

social_clust_k=cbind(S, clust_k$cluster)
plotcluster(S, clust_k$cluster)
```

Re-visualizing the same plot

```{r 4_6_2}
kmeans_2 = kmeans(S,4,nstart=25)
fviz_cluster(kmeans_2,data=S,geom=c('point'),ellipse.type='euclid')
```

### Plotting the clusters

Calculating mu and sigma

```{r 4_7_1,warning=FALSE}
mu = attr(scaled_s,"scaled:center")
sigma = attr(scaled_s,"scaled:scale")

social_clust_k_1 = as.data.frame(cbind(clust_k$center[1,]*sigma + mu, 
                                       clust_k$center[2,]*sigma + mu,
                                       clust_k$center[3,]*sigma + mu,
                                       clust_k$center[4,]*sigma + mu))

names(social_clust_k_1)=c('Cluster_1','Cluster_2','Cluster_3','Cluster_4')
#,'Cluster_5')
social_clust_k_1$type=row.names(social_clust_k_1)
summary(social_clust_k_1)
```


Cluster 1

````{r 4_7_2}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
       x ="Category", y = "Cluster centre values") 
```

Cluster 2

```{r 4_7_3}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
       x ="Category", y = "Cluster centre values")
```

Cluster 3

```{r 4_7_4}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
       x ="Category", y = "Cluster centre values")
```

Cluster 4

```{r 4_7_5}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
       x ="Category", y = "Cluster centre values")
```


## Result
Based on this analysis, we have identified the following market segments:  
- 1. Current Events, Travel, Computers  
- 2. Health/Nutrition, Cooking, Personal Fitness   
- 3. Sports Fandom, Politics  
- 4. College_uni,news,online_gaming  

Following are certain inferences based on each segment:  
- 1. The Informed - Loves to stay ahead of things and well read about world events
                    Middle aged   
                    
- 2. The fitness enthusiasts - Pretty self evident, like to watch their food intake
                               and love breaking a sweat. Young.   
                               
- 3. The Average Joe - Stays out of most discussions, but can go on for a long time
                       about their favorite team on and off the field. Middle aged
                       to old.   
                       
- 4. The Student - College going students who like to unwind online after a day's
                   worth of hard work. Young.   


# Question 5- Author Attribution

```{r, echo = FALSE,warning=FALSE,include=FALSE}
# Import Libraries
library(tm) 
library(slam)
library(ggplot2)
library(caret)
library(plyr)
library(magrittr)
library(proxy)
library('e1071')
library(dplyr)
```


```{r, echo = FALSE,warning=FALSE,include=FALSE}
# Create Read Function
read = function(fname){read(elem=list(content=readLines(fname)), id=fname, language='en') }
```
```{r}
pathstrTrain = 'ReutersC50/C50train/*'
pathstrTest = 'ReutersC50/C50test/*'
```


```{r}	
# Reading in the train folders
train=Sys.glob(pathstrTrain)
```

```{r}
# Make train set
combined=NULL
labels=NULL
for (x in train)
{ 
  author=substring(x,first=length(pathstrTrain))
  article=Sys.glob(paste0(x,'/*.txt'))
  combined=append(combined,article)
  labels=append(labels,rep(author,length(article)))
}
```

```{r}
# Clean train file names
reader =function(fname)
  {
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') 
  }
comb = lapply(combined, reader) 
names(comb) = combined
names(comb) = sub('.txt', '', names(comb))
``` 

```{r}
# Text-Mining Corpus
corp_train=Corpus(VectorSource(comb))
```


```{r, echo = FALSE,warning=FALSE}
# TM_Map to pre-process
corp_train_cp = corp_train 
corp_train_cp = tm_map(corp_train_cp, content_transformer(removeNumbers)) 
corp_train_cp = tm_map(corp_train_cp, content_transformer(tolower)) 
corp_train_cp = tm_map(corp_train_cp, content_transformer(removePunctuation))
corp_train_cp = tm_map(corp_train_cp, content_transformer(removeWords),stopwords("en")) 
corp_train_cp = tm_map(corp_train_cp, content_transformer(stripWhitespace)) 
DTM_train = DocumentTermMatrix(corp_train_cp)
DTM_train # Basic summary statistics
DTM_train1 =removeSparseTerms(DTM_train,.99) # Remove sparseterms from data
tf_idf_mat = weightTfIdf(DTM_train1)
DTM_train2=as.matrix(tf_idf_mat) 
# tf_idf_mat
```
2500 documents and 3397 terms found

```{r}
# Reading in the test folders
test=Sys.glob(pathstrTest)
```

```{r}
# Make test set
combined1=NULL
labels1=NULL
for (x in test)
{ 
  author1=substring(x,first=length(pathstrTest))
  article1=Sys.glob(paste0(x,'/*.txt'))
  combined1=append(combined1,article1)
  labels1=append(labels1,rep(author1,length(article1)))
}
``` 

```{r}
# Clean test file names
comb1 = lapply(combined1, reader) 
names(comb1) = combined1
names(comb1) = sub('.txt', '', names(comb1))
```

```{r}
# Text-Mining Corpus
corp_test=Corpus(VectorSource(comb1))
```

```{r, echo = FALSE,warning=FALSE,include=FALSE}
# TM_Map to pre-process
corp_test_cp=corp_test 
corp_test_cp = tm_map(corp_test_cp, content_transformer(removeNumbers)) 
corp_test_cp = tm_map(corp_test_cp, content_transformer(tolower)) 
corp_test_cp = tm_map(corp_test_cp, content_transformer(removePunctuation)) 
corp_test_cp = tm_map(corp_test_cp, content_transformer(removeWords),stopwords("en")) 
corp_test_cp = tm_map(corp_test_cp, content_transformer(stripWhitespace)) 
```


```{r, echo = FALSE,warning=FALSE}
# Specify column names by DocumentTermMatrix to make sure same column names are in the train document and matrix
DTM_test=DocumentTermMatrix(corp_test_cp,list(dictionary=colnames(DTM_train)))
tf_idf_mat_ts = weightTfIdf(DTM_test)
DTM_test2=as.matrix(tf_idf_mat_ts) 
# tf_idf_mat_ts 
```
2500 documents and 32570 terms found

```{r}
DTM_train3 =DTM_train2[,which(colSums(DTM_train2) != 0)] 
DTM_test3 =DTM_test2[,which(colSums(DTM_test2) != 0)]
```


```{r}
DTM_test3 = DTM_test3[,intersect(colnames(DTM_test3),colnames(DTM_train3))]
DTM_train3 = DTM_train3[,intersect(colnames(DTM_test3),colnames(DTM_train3))]
```


```{r}
model_pca = prcomp(DTM_train3,scale=TRUE)
predict_pca = predict(model_pca,newdata = DTM_test3)
```


```{r}
# At PC729 75% of variance accounted for, so stopping at that value
plot(model_pca,type='line') 
var =apply(model_pca$x, 2, var)  
prop =var / sum(var)
# cumsum(prop)
plot(cumsum(model_pca$sdev^2/sum(model_pca$sdev^2)), xlab = 'terms', ylab = 'Percent variance explained')
```


```{r}
train_class = data.frame(model_pca$x[,1:729])
train_class['author']=labels
train_load = model_pca$rotation[,1:729]
test_class_predict =scale(DTM_test3) %*% train_load
test_class =as.data.frame(test_class_predict)
test_class['author']=labels1
```

## Model 1: Naive Bayes  
```{r}
library('e1071')
model_naive=naiveBayes(as.factor(author)~.,data=train_class)
predict_naive=predict(model_naive,test_class)
``` 


```{r}
library(caret)
predicted_nb=predict_naive
actual_nb=as.factor(test_class$author)
temp_nb=as.data.frame(cbind(actual_nb,predicted_nb))
temp_nb$flag=ifelse(temp_nb$actual_nb==temp_nb$predicted_nb,1,0)
sum(temp_nb$flag)
nb_sum <- sum(temp_nb$flag)*100/nrow(temp_nb)
sprintf("Accuracy: %0.2f percent", nb_sum)
```

```{r, echo = FALSE,warning=FALSE,include=FALSE}
predict_naive_train=predict(model_naive,train_class)
train_error_naive_predict=predict_naive
```

## Model 2: K-Nearest Neighbors  
```{r}
train.X = subset(train_class, select = -c(author))
test.X = subset(test_class,select = -c(author))
train.author=as.factor(train_class$author)
test.author=as.factor(test_class$author)
```


```{r}
library(class)
set.seed(1)
knn_predict=knn(train.X,test.X,train.author,k=10)
```

```{r}
temp_knn=as.data.frame(cbind(knn_predict,test.author))
temp_knn_flag=ifelse(as.integer(knn_predict)==as.integer(test.author),1,0)
knn_sum = sum(temp_knn_flag)*100/nrow(temp_knn) #850
sprintf("Accuracy: %0.2f percent", knn_sum)
```

## Model 3: Random Forest   
```{r, echo = FALSE,warning=FALSE,include=FALSE}
library(randomForest)
set.seed(1)
model_rand=randomForest(as.factor(author)~.,data=train_class, mtry=5,importance=TRUE)
```


```{r}
predict_rand=predict(model_rand,data=test_class)
table_rand=as.data.frame(table(predict_rand,as.factor(test_class$author)))
predicted=predict_rand
actual=as.factor(test_class$author)
temp=as.data.frame(cbind(actual,predicted))
temp$flag=ifelse(temp$actual==temp$predicted,1,0)
sum(temp$flag)
rf_sum = sum(temp$flag)*100/nrow(temp)
sprintf("Accuracy: %0.2f percent", rf_sum)
```

## Conclusion:
We performed 3 different classification models for this dataset. Our most accurate model by far was Random Forest. Naive Bayes and KNN had significantly lower accuracies, as seen from the plot below:

Plot of Accuracies:
```{r}
library(ggplot2)
graph=data.frame("Model"=c('Random Forest', 'KNN', 'Naive Bayes'), "Accuracy"=c(rf_sum, knn_sum, nb_sum))
graph
ggplot(graph,aes(x=Model,y=Accuracy)) + geom_col() + ylim(0,100)
```

# Question 6- Association Rule Mining

```{r echo=FALSE, include=FALSE}
library(arules) 
library(tidyverse)
library(arulesViz)
```

```{r echo=FALSE}
groceries = scan("groceries.txt", what = "", sep = "\n")
```

```{r echo=FALSE, include=FALSE}
summary(groceries)
str(groceries)
```

```{r echo=FALSE, include=FALSE}
groceries = strsplit(groceries, ",")
grocery.transactions = as(groceries, "transactions")
summary(grocery.transactions)
```
9835 transactions, whole milk is the most frequent item (2513 transactions) followed by other vegetables (1903 transactions)  
Transaction sizes range from 1 to 32 items, with an average basket size of ~4 items

## Plot of Top 10 Items
```{r echo=FALSE}
itemFrequencyPlot(grocery.transactions, topN = 10)
```
  

This plot shows the top 10 items purchased among all transactions, depicting our previous observations from the summary code above.


## Parameter Selection 1:

To begin, we selected a relatively high support confidence ratio and a low confidence level to see what outputs would occur.

```{r echo=FALSE, include=FALSE}
grocery.1 = apriori(grocery.transactions, parameter=list(support=0.05, confidence=.01, minlen=2))
```

6 rules created given the high support confidence ratio and extremely low confidence level.


```{r echo=FALSE}
arules::inspect(grocery.1)
plot(grocery.1, method='graph')
```


All relationships have whole milk; yogurt, other vegetables, and rolls/buns come up twice. This makes sense given they are all among the most frequently purchased items, as previously shown.


## Parameter Selection 2:

We decreased support confidence ratio and increased confidence levels to see this impact on the output obtained.

```{r echo=FALSE, include=FALSE}
grocery.2 = apriori(grocery.transactions, parameter=list(support=0.02, confidence=.05, minlen=2))
```

128 rules created given the lowered support confidence ratio and slightly higher confidence level.

```{r echo=FALSE}
arules::inspect(grocery.2)
plot(head(grocery.2,15,by='lift'), method='graph')
```

More items are included in this output as a result, but whole milk, yogurt, and other vegetables are still highly present, as shown in this plot. 

## Parameter Selection 3:

We further decreased support confidence ratio and further increased confidence levels to see the magnified impact of these parameters on a larger scale.

```{r echo=FALSE, include=FALSE}
grocery.3 = apriori(grocery.transactions, parameter=list(support=0.002, confidence=0.2, minlen=2))
```
5111 rules created given the even lower support confidence ratio and significantly higher confidence level.

```{r echo=FALSE, include = FALSE}
arules::inspect(grocery.3)
```
```{r}
plot(head(grocery.3, 5, by='lift'), method='graph')
```

This graph shows that transactions that include whole milk are likely to include other baking supplies.
Also shows transactions containing a dairy product like hard cheese will likley include other dairy products.
Depicted is also a relationship between transactions containing salty snacks and popcorn, as well as hamburger meat with instant meals.
These relationships all make sense given the similarities of the items and their respective food categories.

